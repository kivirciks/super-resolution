{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**EDSR**"
      ],
      "metadata": {
        "id": "1HYzu2LzObc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Add, Conv2D, Input, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Усреднение RGB каналов\n",
        "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
        "\n",
        "# Задание модели EDSR\n",
        "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None):\n",
        "    x_in = Input(shape=(None, None, 3))\n",
        "    x = Lambda(normalize)(x_in)\n",
        "\n",
        "    # Задание слоев сверточной нейронной сети\n",
        "    x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
        "    for i in range(num_res_blocks):\n",
        "        b = res_block(b, num_filters, res_block_scaling)\n",
        "    b = Conv2D(num_filters, 3, padding='same')(b)\n",
        "    x = Add()([x, b])\n",
        "\n",
        "    x = upsample(x, scale, num_filters)\n",
        "    x = Conv2D(3, 3, padding='same')(x)\n",
        "\n",
        "    x = Lambda(denormalize)(x)\n",
        "    return Model(x_in, x, name=\"edsr\")\n",
        "\n",
        "# Задание выходных слоев EDSR\n",
        "def res_block(x_in, filters, scaling):\n",
        "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
        "    x = Conv2D(filters, 3, padding='same')(x)\n",
        "    if scaling:\n",
        "        x = Lambda(lambda t: t * scaling)(x)\n",
        "    x = Add()([x_in, x])\n",
        "    return x\n",
        "\n",
        "# Субпиксельная свертка\n",
        "def upsample(x, scale, num_filters):\n",
        "    def upsample_1(x, factor, **kwargs):\n",
        "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
        "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
        "    if scale == 2:\n",
        "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
        "    elif scale == 3:\n",
        "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
        "    elif scale == 4:\n",
        "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
        "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
        "    return x\n",
        "\n",
        "# Перемешивание пикселей\n",
        "def pixel_shuffle(scale):\n",
        "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "# Нормализация\n",
        "def normalize(x):\n",
        "    return (x - DIV2K_RGB_MEAN) / 127.5\n",
        "\n",
        "# Денормализация\n",
        "def denormalize(x):\n",
        "    return x * 127.5 + DIV2K_RGB_MEAN"
      ],
      "metadata": {
        "id": "I1ET5oF7MiO3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**"
      ],
      "metadata": {
        "id": "VU_WTNmTOek1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data.experimental import AUTOTUNE\n",
        "\n",
        "# Задание параметров скачивания датасета\n",
        "class DIV2K:\n",
        "    def __init__(self,\n",
        "                 scale=2,\n",
        "                 subset='train',\n",
        "                 downgrade='bicubic',\n",
        "                 images_dir='.div2k/images',\n",
        "                 caches_dir='.div2k/caches'):\n",
        "\n",
        "        # Задание коэффициента \"ухудшения\" изображения\n",
        "        self._ntire_2018 = True\n",
        "        _scales = [2, 3, 4, 8]\n",
        "        if scale in _scales:\n",
        "            self.scale = scale\n",
        "        else:\n",
        "            raise ValueError(f'scale must be in ${_scales}')\n",
        "\n",
        "        # Разбиение на тестовую и валижационную выборки\n",
        "        if subset == 'train':\n",
        "            self.image_ids = range(1, 801)\n",
        "        elif subset == 'valid':\n",
        "            self.image_ids = range(801, 901)\n",
        "        else:\n",
        "            raise ValueError(\"subset must be 'train' or 'valid'\")\n",
        "\n",
        "        # Задание способа интерполяции (используем бикубическую)\n",
        "        _downgrades_a = ['bicubic', 'unknown']\n",
        "        _downgrades_b = ['mild', 'difficult']\n",
        "\n",
        "        # Установка ограничений на коэффициенты ухудшения изображений и \n",
        "        # способ ухудшения (интерполяция)\n",
        "        if scale == 8 and downgrade != 'bicubic':\n",
        "            raise ValueError(f'scale 8 only allowed for bicubic downgrade')\n",
        "\n",
        "        if downgrade in _downgrades_b and scale != 4:\n",
        "            raise ValueError(f'{downgrade} downgrade requires scale 4')\n",
        "\n",
        "        if downgrade == 'bicubic' and scale == 8:\n",
        "            self.downgrade = 'x8'\n",
        "        elif downgrade in _downgrades_b:\n",
        "            self.downgrade = downgrade\n",
        "        else:\n",
        "            self.downgrade = downgrade\n",
        "            self._ntire_2018 = False\n",
        "\n",
        "        self.subset = subset\n",
        "        self.images_dir = images_dir\n",
        "        self.caches_dir = caches_dir\n",
        "\n",
        "        os.makedirs(images_dir, exist_ok=True)\n",
        "        os.makedirs(caches_dir, exist_ok=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    # Предосбработка датасета\n",
        "    # AUTOTUNE - автоматически подбираемые параметры\n",
        "    def dataset(self, batch_size=16, repeat_count=None, random_transform=True):\n",
        "        ds = tf.data.Dataset.zip((self.lr_dataset(), self.hr_dataset()))\n",
        "        # Распараллеивание потоков предобраюотки изображений\n",
        "        if random_transform:\n",
        "            # Map проходит по каждому изображению и подбирает параметры\n",
        "            ds = ds.map(lambda lr, hr: random_crop(lr, hr, scale=self.scale), num_parallel_calls=AUTOTUNE)\n",
        "            # Изменение вращения изображения\n",
        "            ds = ds.map(random_rotate, num_parallel_calls=AUTOTUNE)\n",
        "            # Изменение угла поворота изображения\n",
        "            ds = ds.map(random_flip, num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.batch(batch_size)\n",
        "        ds = ds.repeat(repeat_count)\n",
        "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    # Параметры для High-Resolution части датасета\n",
        "    def hr_dataset(self):\n",
        "        # Если уже не скачано, но скачивается архив\n",
        "        if not os.path.exists(self._hr_images_dir()):\n",
        "            download_archive(self._hr_images_archive(), self.images_dir, extract=True)\n",
        "        # Сохраняется в кэш\n",
        "        ds = self._images_dataset(self._hr_image_files()).cache(self._hr_cache_file())\n",
        "        # Если не сохранен индекс изображения, то тоже сохраняется\n",
        "        if not os.path.exists(self._hr_cache_index()):\n",
        "            self._populate_cache(ds, self._hr_cache_file())\n",
        "        return ds\n",
        "\n",
        "    # Параметры для Low-Resolution части датасета (аналогично HR)\n",
        "    def lr_dataset(self):\n",
        "        if not os.path.exists(self._lr_images_dir()):\n",
        "            download_archive(self._lr_images_archive(), self.images_dir, extract=True)\n",
        "        ds = self._images_dataset(self._lr_image_files()).cache(self._lr_cache_file())\n",
        "        if not os.path.exists(self._lr_cache_index()):\n",
        "            self._populate_cache(ds, self._lr_cache_file())\n",
        "        return ds\n",
        "\n",
        "    # Сохранение путей по изображений и индексов\n",
        "    def _hr_cache_file(self):\n",
        "        return os.path.join(self.caches_dir, f'DIV2K_{self.subset}_HR.cache')\n",
        "\n",
        "    def _lr_cache_file(self):\n",
        "        return os.path.join(self.caches_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}_X{self.scale}.cache')\n",
        "\n",
        "    def _hr_cache_index(self):\n",
        "        return f'{self._hr_cache_file()}.index'\n",
        "\n",
        "    def _lr_cache_index(self):\n",
        "        return f'{self._lr_cache_file()}.index'\n",
        "\n",
        "    def _hr_image_files(self):\n",
        "        images_dir = self._hr_images_dir()\n",
        "        return [os.path.join(images_dir, f'{image_id:04}.png') for image_id in self.image_ids]\n",
        "\n",
        "    def _lr_image_files(self):\n",
        "        images_dir = self._lr_images_dir()\n",
        "        return [os.path.join(images_dir, self._lr_image_file(image_id)) for image_id in self.image_ids]\n",
        "\n",
        "    def _lr_image_file(self, image_id):\n",
        "        if not self._ntire_2018 or self.scale == 8:\n",
        "            return f'{image_id:04}x{self.scale}.png'\n",
        "        else:\n",
        "            return f'{image_id:04}x{self.scale}{self.downgrade[0]}.png'\n",
        "\n",
        "    def _hr_images_dir(self):\n",
        "        return os.path.join(self.images_dir, f'DIV2K_{self.subset}_HR')\n",
        "\n",
        "    def _lr_images_dir(self):\n",
        "        if self._ntire_2018:\n",
        "            return os.path.join(self.images_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}')\n",
        "        else:\n",
        "            return os.path.join(self.images_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}', f'X{self.scale}')\n",
        "\n",
        "    def _hr_images_archive(self):\n",
        "        return f'DIV2K_{self.subset}_HR.zip'\n",
        "\n",
        "    def _lr_images_archive(self):\n",
        "        if self._ntire_2018:\n",
        "            return f'DIV2K_{self.subset}_LR_{self.downgrade}.zip'\n",
        "        else:\n",
        "            return f'DIV2K_{self.subset}_LR_{self.downgrade}_X{self.scale}.zip'\n",
        "\n",
        "    # Используется, когда мы хотим вернуть одно и то же, независимо от вызываемого дочернего класса\n",
        "    @staticmethod\n",
        "    # Перевод PNG-изображения в наборы векторов (через разложение RGB-каналов)\n",
        "    def _images_dataset(image_files):\n",
        "        ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "        ds = ds.map(tf.io.read_file)\n",
        "        ds = ds.map(lambda x: tf.image.decode_png(x, channels=3), num_parallel_calls=AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    # Сохранение этого в кеше\n",
        "    @staticmethod\n",
        "    def _populate_cache(ds, cache_file):\n",
        "        print(f'Caching decoded images in {cache_file} ...')\n",
        "        for _ in ds: pass\n",
        "        print(f'Cached decoded images in {cache_file}.')\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "#  Предобработка изображений\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Обрезка изображения\n",
        "def random_crop(lr_img, hr_img, hr_crop_size=96, scale=2):\n",
        "    lr_crop_size = hr_crop_size // scale\n",
        "    lr_img_shape = tf.shape(lr_img)[:2]\n",
        "    lr_w = tf.random.uniform(shape=(), maxval=lr_img_shape[1] - lr_crop_size + 1, dtype=tf.int32)\n",
        "    lr_h = tf.random.uniform(shape=(), maxval=lr_img_shape[0] - lr_crop_size + 1, dtype=tf.int32)\n",
        "    hr_w = lr_w * scale\n",
        "    hr_h = lr_h * scale\n",
        "    lr_img_cropped = lr_img[lr_h:lr_h + lr_crop_size, lr_w:lr_w + lr_crop_size]\n",
        "    hr_img_cropped = hr_img[hr_h:hr_h + hr_crop_size, hr_w:hr_w + hr_crop_size]\n",
        "    return lr_img_cropped, hr_img_cropped\n",
        "\n",
        "# Поворот изображений\n",
        "def random_flip(lr_img, hr_img):\n",
        "    rn = tf.random.uniform(shape=(), maxval=1)\n",
        "    return tf.cond(rn < 0.5,\n",
        "                   lambda: (lr_img, hr_img),\n",
        "                   lambda: (tf.image.flip_left_right(lr_img),\n",
        "                            tf.image.flip_left_right(hr_img)))\n",
        "\n",
        "# Вращение изображений\n",
        "def random_rotate(lr_img, hr_img):\n",
        "    rn = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
        "    return tf.image.rot90(lr_img, rn), tf.image.rot90(hr_img, rn)\n",
        "\n",
        "# Путь, откуда скачивается датасет\n",
        "def download_archive(file, target_dir, extract=True):\n",
        "    source_url = f'http://data.vision.ee.ethz.ch/cvl/DIV2K/{file}'\n",
        "    target_dir = os.path.abspath(target_dir)\n",
        "    tf.keras.utils.get_file(file, source_url, cache_subdir=target_dir, extract=extract)\n",
        "    os.remove(os.path.join(target_dir, file))"
      ],
      "metadata": {
        "id": "iYPEk5yZOoNu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Непосредственно получение датасета\n",
        "train = DIV2K(scale=4, downgrade='bicubic', subset='train')\n",
        "train_ds = train.dataset(batch_size=16, random_transform=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmIZ46v6OYcX",
        "outputId": "1fb5f17c-14d8-4265-bbf3-72a20b9ff48d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip\n",
            "246914039/246914039 [==============================] - 19s 0us/step\n",
            "Caching decoded images in .div2k/caches/DIV2K_train_LR_bicubic_X4.cache ...\n",
            "Cached decoded images in .div2k/caches/DIV2K_train_LR_bicubic_X4.cache.\n",
            "Downloading data from http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
            "3530603713/3530603713 [==============================] - 253s 0us/step\n",
            "Caching decoded images in .div2k/caches/DIV2K_train_HR.cache ...\n",
            "Cached decoded images in .div2k/caches/DIV2K_train_HR.cache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "\n",
        "# Создание директории для сохранения весов\n",
        "weights_dir = 'weights/article'\n",
        "os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "# EDSR baseline\n",
        "model_edsr = edsr(scale=4, num_res_blocks=16)\n",
        "\n",
        "#  Оптимизатор Адама с планировщиком, который вдвое снижает скорость обучения после 200 000 шагов\n",
        "optim_edsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-4, 5e-5]))\n",
        "\n",
        "# Компиляция и обучение модели для 300 000 шагов\n",
        "# БЫЛО 300 и 1000\n",
        "model_edsr.compile(optimizer=optim_edsr, loss='mean_absolute_error')\n",
        "model_edsr.fit(train_ds, epochs=50, steps_per_epoch=500)\n",
        "\n",
        "# Сохранение весов\n",
        "model_edsr.save_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_Li0yEO0GM",
        "outputId": "43132175-ddd2-44fa-b2ba-b0e0063c799d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "500/500 [==============================] - 294s 560ms/step - loss: 15.5975\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 281s 563ms/step - loss: 9.0250\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 281s 563ms/step - loss: 8.2870\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.9305\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.7976\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.6443\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.6018\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.5578\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.5228\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.4118\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.3043\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.3882\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 7.2333\n",
            "Epoch 14/50\n",
            "124/500 [======>.......................] - ETA: 3:31 - loss: 7.1794"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model.wdsr import wdsr_b\n",
        "\n",
        "# Custom WDSR B model (0.62M parameters)\n",
        "model_wdsr = wdsr_b(scale=4, num_res_blocks=32)\n",
        "\n",
        "# Adam optimizer with a scheduler that halfs learning rate after 200,000 steps\n",
        "optim_wdsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-3, 5e-4]))\n",
        "\n",
        "# Compile and train model for 300,000 steps with L1 pixel loss\n",
        "model_wdsr.compile(optimizer=optim_wdsr, loss='mean_absolute_error')\n",
        "model_wdsr.fit(train_ds, epochs=300, steps_per_epoch=1000)\n",
        "\n",
        "# Save weights\n",
        "model_wdsr.save_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))"
      ],
      "metadata": {
        "id": "VgjbgUifO6JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import srgan\n",
        "\n",
        "# Used in content_loss\n",
        "mean_squared_error = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Used in generator_loss and discriminator_loss\n",
        "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "# Model that computes the feature map after the 4th convolution \n",
        "# before the 5th max-pooling layer in VGG19. This is layer 20 in\n",
        "# the corresponding Keras model.\n",
        "vgg = srgan.vgg_54()\n",
        "\n",
        "# EDSR model used as generator in SRGAN\n",
        "generator = edsr(scale=4, num_res_blocks=16)\n",
        "generator.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
        "\n",
        "# SRGAN discriminator\n",
        "discriminator = srgan.discriminator()\n",
        "# Optmizers for generator and discriminator. SRGAN will be trained for\n",
        "# 200,000 steps and learning rate is reduced from 1e-4 to 1e-5 after\n",
        "# 100,000 steps\n",
        "schedule = PiecewiseConstantDecay(boundaries=[100000], values=[1e-4, 1e-5])\n",
        "generator_optimizer = Adam(learning_rate=schedule)\n",
        "discriminator_optimizer = Adam(learning_rate=schedule)\n",
        "\n",
        "def generator_loss(sr_out):\n",
        "    return binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
        "\n",
        "def discriminator_loss(hr_out, sr_out):\n",
        "    hr_loss = binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
        "    sr_loss = binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
        "    return hr_loss + sr_loss\n",
        "\n",
        "@tf.function\n",
        "def content_loss(hr, sr):\n",
        "    sr = tf.keras.applications.vgg19.preprocess_input(sr)\n",
        "    hr = tf.keras.applications.vgg19.preprocess_input(hr)\n",
        "    sr_features = vgg(sr) / 12.75\n",
        "    hr_features = vgg(hr) / 12.75\n",
        "    return mean_squared_error(hr_features, sr_features)\n",
        "@tf.function\n",
        "def train_step(lr, hr):\n",
        "    \"\"\"SRGAN training step.\n",
        "    \n",
        "    Takes an LR and an HR image batch as input and returns\n",
        "    the computed perceptual loss and discriminator loss.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        lr = tf.cast(lr, tf.float32)\n",
        "        hr = tf.cast(hr, tf.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        sr = generator(lr, training=True)\n",
        "        hr_output = discriminator(hr, training=True)\n",
        "        sr_output = discriminator(sr, training=True)\n",
        "\n",
        "        # Compute losses\n",
        "        con_loss = content_loss(hr, sr)\n",
        "        gen_loss = generator_loss(sr_output)\n",
        "        perc_loss = con_loss + 0.001 * gen_loss\n",
        "        disc_loss = discriminator_loss(hr_output, sr_output)\n",
        "\n",
        "    # Compute gradient of perceptual loss w.r.t. generator weights \n",
        "    gradients_of_generator = gen_tape.gradient(perc_loss, generator.trainable_variables)\n",
        "    # Compute gradient of discriminator loss w.r.t. discriminator weights \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    # Update weights of generator and discriminator\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return perc_loss, disc_loss\n",
        "\n",
        "pls_metric = tf.keras.metrics.Mean()\n",
        "dls_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "steps = 200000\n",
        "step = 0\n",
        "\n",
        "# Train SRGAN for 200,000 steps.\n",
        "for lr, hr in train_ds.take(steps):\n",
        "    step += 1\n",
        "\n",
        "    pl, dl = train_step(lr, hr)\n",
        "    pls_metric(pl)\n",
        "    dls_metric(dl)\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f'{step}/{steps}, perceptual loss = {pls_metric.result():.4f}, discriminator loss = {dls_metric.result():.4f}')\n",
        "        pls_metric.reset_states()\n",
        "        dls_metric.reset_states()\n",
        "\n",
        "generator.save_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))"
      ],
      "metadata": {
        "id": "1h16hZ_dO8T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WDSR B model used as generator in SRGAN\n",
        "generator = wdsr_b(scale=4, num_res_blocks=32)\n",
        "generator.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))\n",
        "# Run SRGAN training ...\n",
        "generator.save_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4-fine-tuned.h5'))"
      ],
      "metadata": {
        "id": "8fXDD3PiPSop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import resolve_single\n",
        "from utils import load_image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def resolve_and_plot(model_pre_trained, model_fine_tuned, lr_image_path):\n",
        "    lr = load_image(lr_image_path)\n",
        "    \n",
        "    sr_pt = resolve_single(model_pre_trained, lr)\n",
        "    sr_ft = resolve_single(model_fine_tuned, lr)\n",
        "    \n",
        "    plt.figure(figsize=(20, 20))\n",
        "    \n",
        "    model_name = model_pre_trained.name.upper()\n",
        "    images = [lr, sr_pt, sr_ft]\n",
        "    titles = ['LR', f'SR ({model_name}, pixel loss)', f'SR ({model_name}, perceptual loss)']\n",
        "    positions = [1, 3, 4]\n",
        "    \n",
        "    for i, (image, title, position) in enumerate(zip(images, titles, positions)):\n",
        "        plt.subplot(2, 2, position)\n",
        "        plt.imshow(image)\n",
        "        plt.title(title)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        \n",
        "weights_dir = 'weights/article'"
      ],
      "metadata": {
        "id": "Yq4tz1tzPUpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsr_pre_trained = edsr(scale=4, num_res_blocks=16)\n",
        "edsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
        "\n",
        "edsr_fine_tuned = edsr(scale=4, num_res_blocks=16)\n",
        "edsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))\n",
        "\n",
        "resolve_and_plot(edsr_pre_trained, edsr_fine_tuned, 'demo/0869x4-crop.png')"
      ],
      "metadata": {
        "id": "tNfHPup7PaZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.wdsr import wdsr_b\n",
        "\n",
        "wdsr_pre_trained = wdsr_b(scale=4, num_res_blocks=32)\n",
        "wdsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))\n",
        "\n",
        "wdsr_fine_tuned = wdsr_b(scale=4, num_res_blocks=32)\n",
        "wdsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4-fine-tuned.h5'))\n",
        "\n",
        "resolve_and_plot(wdsr_pre_trained, wdsr_fine_tuned, 'demo/0829x4-crop.png')"
      ],
      "metadata": {
        "id": "e-8T3ORMPeuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipPMw979Pgyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}